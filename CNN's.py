# -*- coding: utf-8 -*-
"""Cópia de redes_neurais_convolucionais.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BA_bzrTAL0G2D1B93xcinfdg1dc0D1Nu

#Redes Neurais Convolucionais

As Redes Neurais Convolucionais tem sido amplamente utilizadas nos últimos anos devido aos avanços computacionais e da inteligência artificial. É uma técnica utilizada para análise de imagem, a qual capta uma imagem de entrada, atribui pesos em vários aspectos da imagem e é capaz de diferenciar uma das outras. Tem a característica de ser uma arquitetura de aprendizado profundo que possui a operação de convolução em pelo menos uma camada.

Alguns exemplos são a detecção de cáries por radiografias e diagnóstico  do câncer de pele por meio de imagens dermatoscópicas.

Como são redes especializadas em análise e classificação de imagens, elas recebem vetores de valores correspondentes as intensidades dos pixels na imagem. Ou seja, as entradas são compostas por matrizes tridimensionais com a altura, largura e a profundidade da imagem (que é determinada pelas cores).

Uma das grandes vantagens de se utilizar esse tipo de metodologia, em relação a redes neurais artificiais, está no número de parâmetros após as camadas de convoluções. Com isso, há uma redução no custo computacional para rodar os modelos.

A seguir apresentaremos uma arquitetura geral de uma CNN.
"""

import matplotlib.pyplot as plt
import cv2

plt.figure(figsize = (20,5))
image = cv2.imread("/content/arq_geral.jpg")
plt.imshow(image)

"""Na imagem acima são apresentados os passos de uma Rede Neural Convolucional. Inicialmente, tem-se as imagens de entrada. Em seguida, há uma operação de convolução seguida de uma operação de pooling. Após essas operações, há um processo de achatamento e por fim tem-se uma camada totalmente conectada.

A convolução é um processo matemático que pode ser entendido como a ação de movimentar um filtro (também conhecido como kernel) sobre todas as posições da imagem, realizando o produto escalar entre o kernel e a imagem. O resultado desse processo é chamado mapa de recurso. 

Abaixo é apresentado um processo de convolução, onde o filtro percorre a imagem e gera o mapa de recurso. A profundidade da saída de uma convolução corresponde ao número de filtros que foram aplicados.
"""

plt.figure(figsize = (20,5))
image = cv2.imread("/content/operacaodeconvolucao.jpg")
plt.imshow(image)

"""Além do filtro,  a saída do processo de convolução depende de mais dois fatores, o padding e o stride. 

O stride ou tamanho do passo,  denota o número de pixels pelos quais o filtro se movimenta após cada operação, ou seja, o filtro na  Figura fig:conv se movimentou em 1 pixel a cada operação porque o stride era igual a 1. 

E o padding insere simetricamente novos pixels ao redor da imagem
de entrada.

Após a camada de convolução normalmente há uma camada de pooling, que tem o objetivo de reduzir a dimensão dos mapas de recursos. A camada de pooling atua de forma similar a camada de convolução, mas o stride tem tamanho igual ao tamanho do filtro. A seguir é apresentado um exemplo de dois tipos de pooling.
"""

plt.figure(figsize = (20,5))
image = cv2.imread("/content/polling.jpg")
plt.imshow(image)

"""A Figura acima exemplifica os dois tipos de pooling considerando um pooling de tamanho $2 \times 2$. A esquerda, no pooling máximo, para os quadrados verdes tem-se que o pooling máximo é:
Máximo(3, 7, 10, 8) =10. O pooling médio é calculado de forma análoga.

Para ilustrar a técnica utilizaremos a base de dados MNIST, que contém dígitos manuscritos e é muito utilizada como exemplo para Redes Neurais Convolucionais.

Está disponível em: http://yann.lecun.com/exdb/mnist/

#Pacotes utilizados
"""

import tensorflow as tf 
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import confusion_matrix
from keras.models import Sequential #importando os keras necessarios para os modelos e camadas
from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D
from tensorflow.keras.callbacks import EarlyStopping

"""#Carregando a base de dados"""

(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()

"""Visualizando imagens da base de dados

Essa base é composta por Digitos de 0 a 9, o nosso objetivo nesse exemplo é criar uma Rede Neural Convolucional que seja capaz de indentificar o digito que está nas imagens.
"""

plt.imshow(x_train[1000], cmap='Greys')

"""Dimensões dos conjuntos de treinamento e teste"""

x_train.shape

x_test.shape

"""então temos 60 mil imagens de treino e 10 mil imagens de teste

#Normalização dos dados e redimensionamento

Teremos que redimensionar os dados, pois imagens tem um formato 3D da (x,y,z) como vimos anteriormente há apenas (28,28). Ainda, vamos normalizar os dados.
"""

x_train = x_train.reshape(x_train.shape[0], 28, 28, 1) # colocando pra dim 4
x_test = x_test.reshape(x_test.shape[0], 28, 28, 1) # colocando pra dim 4

x_train.shape

x_test.shape

"""Como as cores nos pixels estão entre 0 e 255 vamos normalizar para transformar em uma escala de 0 a 1"""

x_train = x_train.astype('float32')
x_test = x_test.astype('float32')
x_train /= 255 #normalizando
x_test /= 255 #normalizando

"""#Construindo uma CNN

Primeiro vamos definir o modelo
"""

model = Sequential()

"""Agora vamos adicionar a camada convolucional


"""

model.add(Conv2D(28, kernel_size=(3,3), input_shape=(28, 28, 1))) #definimos um kernel como dimensões 3x3

"""Adicionando uma camada de Max pooling """

model.add(MaxPooling2D(pool_size=(2, 2)))  # com um filtro de tamanho 2x2

"""Agora vamos colocar a camada de achatamento"""

model.add(Flatten())

"""E uma camada totalmente conectada em seguida com um dropout de 20%"""

model.add(Dense(256, activation=tf.nn.relu))
model.add(Dropout(0.2))
model.add(Dense(128, activation=tf.nn.relu))

"""E por fim uma camada com 10 neuronios pois queremos classificar número de 0 a 9"""

model.add(Dense(10,activation=tf.nn.softmax))

"""Agora rodamos o modelo apois definir a função de perda, as metricas de saida e o otimizador."""

model.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])
model.summary()

history = model.fit(x=x_train,y=y_train, epochs=100, initial_epoch = 0, validation_data = (x_test, y_test),callbacks=EarlyStopping(monitor="val_loss", mode="min", verbose=1, patience=5))

"""Agora  podemos pedir a perda e a acurácia final após o treino do modelo que é dada abaixo"""

model.evaluate(x_test, y_test)

"""calculando os valores preditos, temos:"""

y_predito = model.predict(x_test); y_predito

y_predito =np.argmax(y_predito,axis =1)

"""E agora podemos construir a matriz de confusão com os preditos que foram calculados acima e o verdadeiro valor do y."""

cm = confusion_matrix(y_test,y_predito)
cm

"""Gráficos

A seguir vamos fazer os gráficos da perda e da acurácia durante o treinamento e na validação.
"""

plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0,9), history.history["loss"], label="Perda no Treino")
plt.plot(np.arange(0,9), history.history["val_loss"], label="Perda na Validação")
plt.title("Função de Perda")
plt.xlabel("Épocas")
plt.ylabel("Perda")
plt.legend()
plt.show()

"""é possivel notar que conforme as epocas passam a perda no treino diminui e a perda na validação também apesar de haver um aumento na epoca 7."""

plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0,9), history.history["accuracy"], label="Acurácia do Treinamento")
plt.plot(np.arange(0,9), history.history["val_accuracy"], label="Acurácia da Validação")
plt.title("Acurácia")
plt.xlabel("Épocas")
plt.ylabel("Acurácia")
plt.legend()

"""Para acurácia também é possivel notar que aumentou conforme as epocas para o treinamento e na validação também.

#Adicionando una nova camada convolucional  na rede
"""

model2 = Sequential()
model2.add(Conv2D(28, kernel_size=(3,3), input_shape=(28, 28, 1))) #definimos um kernel como dimensões 3x3
model2.add(MaxPooling2D(pool_size=(2, 2)))  # com um filtro de tamanho 2x2
model2.add(Conv2D(14, kernel_size=(3,3))) #definimos um kernel como dimensões 3x3
model2.add(MaxPooling2D(pool_size=(2, 2)))  # com um filtro de tamanho 2x2
model2.add(Flatten()) 
model2.add(Dense(256, activation=tf.nn.relu))
model2.add(Dropout(0.2))
model2.add(Dense(128, activation=tf.nn.relu))
model2.add(Dense(10,activation=tf.nn.softmax))

model2.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])
model2.summary()

history2 = model2.fit(x=x_train,y=y_train, epochs=100, initial_epoch = 0, validation_data = (x_test, y_test),callbacks=EarlyStopping(monitor="val_loss", mode="min", verbose=1, patience=5))

model2.evaluate(x_test, y_test)

y_predito2 = model.predict(x_test)
y_predito2 =np.argmax(y_predito2,axis =1)

cm = confusion_matrix(y_test,y_predito2)
cm

plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0,11), history2.history["loss"], label="Perda no Treino")
plt.plot(np.arange(0,11), history2.history["val_loss"], label="Perda na Validação")
plt.title("Função de Perda")
plt.xlabel("Épocas")
plt.ylabel("Perda")
plt.legend()
plt.show()

plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0,11), history2.history["accuracy"], label="Acurácia do Treinamento")
plt.plot(np.arange(0,11), history2.history["val_accuracy"], label="Acurácia da Validação")
plt.title("Acurácia")
plt.xlabel("Épocas")
plt.ylabel("Acurácia")
plt.legend()

"""# Adicionando mais uma camada convolucional

"""

model3 = Sequential()
model3.add(Conv2D(32, kernel_size=(3,3), input_shape=(28, 28, 1))) #definimos um kernel como dimensões 3x3
model3.add(MaxPooling2D(pool_size=(2, 2)))  # com um filtro de tamanho 2x2
model3.add(Conv2D(16, kernel_size=(3,3))) #definimos um kernel como dimensões 3x3
model3.add(MaxPooling2D(pool_size=(2, 2)))  # com um filtro de tamanho 2x2
model3.add(Conv2D(16, kernel_size=(2,2))) #definimos um kernel como dimensões 3x3
model3.add(Flatten()) 
model3.add(Dense(256, activation=tf.nn.relu))
model3.add(Dropout(0.2))
model3.add(Dense(128, activation=tf.nn.relu))
model3.add(Dense(10,activation=tf.nn.softmax))

model3.compile(optimizer='adam', 
              loss='sparse_categorical_crossentropy', 
              metrics=['accuracy'])
model3.summary()

history3 = model3.fit(x=x_train,y=y_train, epochs=100, initial_epoch = 0, validation_data = (x_test, y_test),callbacks=EarlyStopping(monitor="val_loss", mode="min", verbose=1, patience=5))

model3.evaluate(x_test, y_test)

y_predito3 = model.predict(x_test)
y_predito3 =np.argmax(y_predito3,axis =1)

cm = confusion_matrix(y_test,y_predito3)
cm

plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0,13), history3.history["loss"], label="Perda no Treino")
plt.plot(np.arange(0,13), history3.history["val_loss"], label="Perda na Validação")
plt.title("Função de Perda")
plt.xlabel("Épocas")
plt.ylabel("Perda")
plt.legend()
plt.show()

plt.style.use("ggplot")
plt.figure()
plt.plot(np.arange(0,13), history3.history["accuracy"], label="Acurácia do Treinamento")
plt.plot(np.arange(0,13), history3.history["val_accuracy"], label="Acurácia da Validação")
plt.title("Acurácia")
plt.xlabel("Épocas")
plt.ylabel("Acurácia")
plt.legend()